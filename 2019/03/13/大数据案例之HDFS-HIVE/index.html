<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>大数据案例之HDFS-HIVE | 我的学习记录</title><meta name="description" content="大数据案例之HDFS-HIVE"><meta name="keywords" content="Hadoop,Sqoop,Hive,Python"><meta name="author" content="陈 武"><meta name="copyright" content="陈 武"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://updatecg.oss-cn-beijing.aliyuncs.com/msofficexp.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://www.updatecg.xin/2019/03/13/大数据案例之HDFS-HIVE/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="大数据案例之HDFS-HIVE"><meta name="twitter:description" content="大数据案例之HDFS-HIVE"><meta name="twitter:image" content="https://updatecg.oss-cn-beijing.aliyuncs.com/timg.jpg"><meta property="og:type" content="article"><meta property="og:title" content="大数据案例之HDFS-HIVE"><meta property="og:url" content="http://www.updatecg.xin/2019/03/13/大数据案例之HDFS-HIVE/"><meta property="og:site_name" content="我的学习记录"><meta property="og:description" content="大数据案例之HDFS-HIVE"><meta property="og:image" content="https://updatecg.oss-cn-beijing.aliyuncs.com/timg.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="大数据案例之HDFS-HIVE-Spark" href="http://www.updatecg.xin/2019/03/21/大数据案例之HDFS-HIVE-Spark/"><link rel="next" title="sqoop安装部署问题事项" href="http://www.updatecg.xin/2019/03/13/sqoop安装部署问题事项/"><meta name="baidu-site-verification" content="2zNdXBNuWf"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"http://www.updatecg.xin/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#数据采集"><span class="toc-number">1.</span> <span class="toc-text">数据采集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据处理"><span class="toc-number">2.</span> <span class="toc-text">数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CSV-数据加载入Hadoop-部分代码"><span class="toc-number">2.1.</span> <span class="toc-text">CSV 数据加载入Hadoop  部分代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#将dfs文件加载入hive-部分代码"><span class="toc-number">2.2.</span> <span class="toc-text">将dfs文件加载入hive 部分代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#利用外部表加载dfs数据至分区表"><span class="toc-number">2.2.1.</span> <span class="toc-text">利用外部表加载dfs数据至分区表</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#外部表好处"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">外部表好处</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive创建外部表"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">Hive创建外部表</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HIVE分析数据"><span class="toc-number">2.3.</span> <span class="toc-text">HIVE分析数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#案列-：统计服装制造商主要城市分布-（因为hive字段与值对应错乱，但是导入至mysql不会错乱）"><span class="toc-number">2.3.1.</span> <span class="toc-text">案列 ：统计服装制造商主要城市分布 （因为hive字段与值对应错乱，但是导入至mysql不会错乱）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sqoop-将分析后HIVE数据导出至MYSQL-主要思想："><span class="toc-number">2.4.</span> <span class="toc-text">Sqoop 将分析后HIVE数据导出至MYSQL 主要思想：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#统计-分析"><span class="toc-number">3.</span> <span class="toc-text">统计/分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#爬取今日头条"><span class="toc-number">3.1.</span> <span class="toc-text">爬取今日头条</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HIVE数据分析"><span class="toc-number">3.2.</span> <span class="toc-text">HIVE数据分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#按时间动态分区"><span class="toc-number">3.2.1.</span> <span class="toc-text">按时间动态分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#为外部表挂载分区"><span class="toc-number">3.2.2.</span> <span class="toc-text">为外部表挂载分区</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://updatecg.oss-cn-beijing.aliyuncs.com/wallhaven-2k3k39.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">我的学习记录</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="https://updatecg.oss-cn-beijing.aliyuncs.com/timg.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description">决定一个人有多优秀的，并不是看他有多努力，而是看这个人的思维模式。</div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a><a class="site-page" href="/2016/10/01/chenwu/"><i class="fa-fw fa fa-smile-o"></i><span> 关于</span></a><a class="site-page" href="/messageBoard/"><i class="fa-fw fa fa-coffee"></i><span> 留言板</span></a><a class="site-page" href="/2017/08/25/路上/"><i class="fa-fw fa fa-globe"></i><span> 旅行</span></a><a class="site-page" href="https://www.toolfk.com/"><i class="fa-fw fa fa-ambulance"></i><span> 工具库</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">大数据案例之HDFS-HIVE</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-03-13<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-05-28</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/大数据/">大数据</a></span><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2,612</span><span class="post-meta__separator">|</span><span>阅读时长: 12 分钟</span><span class="post-meta__separator">|</span><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><blockquote>
<p>基于Hdfs、hive、mysql数据处理案例，闲时自玩项目</p>
</blockquote>
<h2 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h2><blockquote>
<p>数据采集方式有很多种，一般在项目中采用数据上报方式。本地为了方便测试则采用读取csv文件。后续python自动抓取数据。</p>
</blockquote>
<p>链接: <a href="https://pan.baidu.com/s/1cOCe1GXAxtkXCUbvY0MWFw" target="_blank" rel="noopener">https://pan.baidu.com/s/1cOCe1GXAxtkXCUbvY0MWFw</a> 提取码: r23c<br>数据量不多，侧重于功能</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><blockquote>
<p>清洗数据,统计分析数据，结果存储HDFS ,加载至HIVE, Sqoop至MYSQL</p>
</blockquote>
<h3 id="CSV-数据加载入Hadoop-部分代码"><a href="#CSV-数据加载入Hadoop-部分代码" class="headerlink" title="CSV 数据加载入Hadoop  部分代码"></a>CSV 数据加载入Hadoop  部分代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public String transfer(File file, String folderPath, String fileName) throws Exception &#123;</span><br><span class="line">  if (!opened) &#123;</span><br><span class="line">      throw new Exception(&quot;FileSystem was not opened!&quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  boolean folderCreated = fs.mkdirs(new Path(folderPath));</span><br><span class="line"></span><br><span class="line">  Path filePath = new Path(folderPath, StrUtils.isEmpty(fileName) ? file.getName() : fileName);</span><br><span class="line">  boolean fileCreated = fs.createNewFile(filePath);</span><br><span class="line"></span><br><span class="line">  FSDataOutputStream append = fs.append(filePath);</span><br><span class="line">  byte[] bytes = new byte[COPY_BUFFERSIZE];</span><br><span class="line">  int size = 0;</span><br><span class="line">  FileInputStream fileInputStream = new FileInputStream(file);</span><br><span class="line">  while ((size = fileInputStream.read(bytes)) &gt; 0) &#123;</span><br><span class="line">      append.write(bytes, 0, size);</span><br><span class="line">  &#125;</span><br><span class="line">  fileInputStream.close();</span><br><span class="line">  return filePath.toUri().toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="将dfs文件加载入hive-部分代码"><a href="#将dfs文件加载入hive-部分代码" class="headerlink" title="将dfs文件加载入hive 部分代码"></a>将dfs文件加载入hive 部分代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  //表</span><br><span class="line">  String yyyyMMdd = hiveTable + DateUtil.formatDate(new Date(), &quot;yyyyMMdd&quot;);</span><br><span class="line">  //参数</span><br><span class="line">  Map&lt;String, String&gt; map = new HashMap&lt;&gt;();</span><br><span class="line">  map.put(&quot;title&quot;, &quot;STRING&quot;);</span><br><span class="line">  map.put(&quot;discountPrice&quot;, &quot;STRING&quot;);</span><br><span class="line">  map.put(&quot;price&quot;, &quot;STRING&quot;);</span><br><span class="line">  map.put(&quot;address&quot;, &quot;STRING&quot;);</span><br><span class="line">  map.put(&quot;count&quot;, &quot;STRING&quot;);</span><br><span class="line"></span><br><span class="line">  //创建表 按天分表</span><br><span class="line">  hiveDataService.createHiveTable(yyyyMMdd, map);</span><br><span class="line">  //将dfs数据加载到hive表</span><br><span class="line">  hiveDataService.loadHiveIntoTable(fs.getDfsPath(), yyyyMMdd);</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * @param tableName     hive表名</span><br><span class="line">  * @param parametersMap 表字段值/类型</span><br><span class="line">  */</span><br><span class="line"> @Override</span><br><span class="line"> public void createHiveTable(String tableName, Map&lt;String, String&gt; parametersMap) &#123;</span><br><span class="line">     StringBuffer sql = new StringBuffer(&quot;CREATE TABLE IF NOT EXISTS &quot;);</span><br><span class="line">     sql.append(&quot;&quot; + tableName + &quot;&quot;);</span><br><span class="line">     StringBuffer sb = new StringBuffer();</span><br><span class="line">     parametersMap.forEach((k, v) -&gt; &#123;</span><br><span class="line">         sb.append(k + &quot; &quot; + v + &quot;,&quot;);</span><br><span class="line">     &#125;);</span><br><span class="line">     sql.append(&quot;(&quot; + sb.deleteCharAt(sb.length() - 1) + &quot;)&quot;);</span><br><span class="line">     sql.append(&quot;ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; LINES TERMINATED BY &apos;\n&apos; &quot;); // 定义分隔符</span><br><span class="line">     sql.append(&quot;STORED AS TEXTFILE&quot;); // 作为文本存储</span><br><span class="line"></span><br><span class="line">     Log.info(&quot;Create table [&quot; + tableName + &quot;] successfully...&quot;);</span><br><span class="line">     try &#123;</span><br><span class="line">         hiveJdbcTemplate.execute(sql.toString());</span><br><span class="line">     &#125; catch (DataAccessException dae) &#123;</span><br><span class="line">         Log.error(dae.fillInStackTrace());</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @param filePath  dfs文件路径</span><br><span class="line"> * @param tableName 表名</span><br><span class="line"> */</span><br><span class="line">@Override</span><br><span class="line">public void loadHiveIntoTable(String filePath, String tableName) &#123;</span><br><span class="line">    StringBuffer sql = new StringBuffer(&quot;load data inpath &quot;);</span><br><span class="line">    sql.append(&quot;&apos;&quot; + filePath + &quot;&apos;into table &quot; + tableName);</span><br><span class="line">    Log.info(&quot;Load data into table successfully...&quot;);</span><br><span class="line">    try &#123;</span><br><span class="line">        hiveJdbcTemplate.execute(sql.toString());</span><br><span class="line">    &#125; catch (DataAccessException dae) &#123;</span><br><span class="line">        Log.error(dae.fillInStackTrace());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="利用外部表加载dfs数据至分区表"><a href="#利用外部表加载dfs数据至分区表" class="headerlink" title="利用外部表加载dfs数据至分区表"></a>利用外部表加载dfs数据至分区表</h4><blockquote>
<p>上述代码中有一步为load data 至hive。在于朋友交流中，他提醒可以直接利用<code>外部加载数据</code>，自此代码如下：</p>
</blockquote>
<h5 id="外部表好处"><a href="#外部表好处" class="headerlink" title="外部表好处"></a><code>外部表</code>好处</h5><ul>
<li>hive创建外部表时,仅记录数据所在的路径,不对数据的位置做任何改变</li>
<li>删除表的时候,外部表只删除元数据,不删除数据</li>
<li>内部表drop表会把元数据删除</li>
</ul>
<h5 id="Hive创建外部表"><a href="#Hive创建外部表" class="headerlink" title="Hive创建外部表"></a>Hive创建外部表</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---------------------------------java代码-----------------------------------------</span><br><span class="line">    /**</span><br><span class="line">     * 利用外部表加载数据   </span><br><span class="line">     *</span><br><span class="line">     * @param tableName     hive表名</span><br><span class="line">     * @param parametersMap 表字段值/类型</span><br><span class="line">     * @param dfsUrl        dfs文件地址</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public synchronized void createOuterHiveTable(String tableName, Map&lt;String, String&gt; parametersMap, String dfsUrl) &#123;</span><br><span class="line">        StringBuffer sql = new StringBuffer(&quot;CREATE EXTERNAL TABLE IF NOT EXISTS &quot;);</span><br><span class="line">        sql.append(&quot;&quot; + tableName + &quot;&quot;);</span><br><span class="line">        StringBuffer sb = new StringBuffer();</span><br><span class="line">        parametersMap.forEach((k, v) -&gt; &#123;</span><br><span class="line">            sb.append(k + &quot; &quot; + v + &quot;,&quot;);</span><br><span class="line">        &#125;);</span><br><span class="line">        sql.append(&quot;(&quot; + sb.deleteCharAt(sb.length() - 1) + &quot;)&quot;);</span><br><span class="line">        sql.append(&quot; PARTITIONED BY (day STRING)&quot;);</span><br><span class="line">        sql.append(&quot; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; &quot; +</span><br><span class="line">                &quot; COLLECTION ITEMS TERMINATED BY &apos;\\002&apos;&quot; +</span><br><span class="line">                &quot; MAP KEYS TERMINATED BY &apos;\\003&apos;&quot; +</span><br><span class="line">                &quot; LINES TERMINATED BY &apos;\n&apos; &quot;); // 定义分隔符</span><br><span class="line">        sql.append(&quot;LOCATION &apos;&quot; + dfsUrl + &quot;&apos;&quot;); // 外部表加载hdfs数据目录</span><br><span class="line"></span><br><span class="line">        Log.info(&quot;Create EXTERNAL table [&quot; + tableName + &quot;] successfully...&quot;);</span><br><span class="line">        try &#123;</span><br><span class="line">            hiveJdbcTemplate.execute(sql.toString());</span><br><span class="line">        &#125; catch (DataAccessException dae) &#123;</span><br><span class="line">            Log.error(dae.fillInStackTrace());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">------------------------------------Sql---------------------------------------------</span><br><span class="line">    CREATE EXTERNAL TABLE IF NOT EXISTS  xx_outer_partitioned</span><br><span class="line">    (</span><br><span class="line">    	affiliatedbasenum STRING,</span><br><span class="line">    	locationid STRING,</span><br><span class="line">    	pickupdate</span><br><span class="line">    	dispatchingbasenum STRING</span><br><span class="line">    )</span><br><span class="line">    PARTITIONED BY (day STRING)</span><br><span class="line">    ROW FORMAT DELIMITED</span><br><span class="line">    	FIELDS TERMINATED BY &apos;,&apos;</span><br><span class="line">    	COLLECTION ITEMS TERMINATED BY &apos;\002&apos;</span><br><span class="line">    	MAP KEYS TERMINATED BY &apos;\003&apos;</span><br><span class="line">    	LINES TERMINATED BY &apos;\n&apos;</span><br><span class="line">    LOCATION &apos;/data/outerClientSummary/&apos;;</span><br></pre></td></tr></table></figure>
<h3 id="HIVE分析数据"><a href="#HIVE分析数据" class="headerlink" title="HIVE分析数据"></a>HIVE分析数据</h3><blockquote>
<p>hive支持sql操作（支持连表操作、排序），支持分区（此功能特别实用，比如数据量庞大时一般会按照天分表，此时就可以利用按天分区）</p>
</blockquote>
<h4 id="案列-：统计服装制造商主要城市分布-（因为hive字段与值对应错乱，但是导入至mysql不会错乱）"><a href="#案列-：统计服装制造商主要城市分布-（因为hive字段与值对应错乱，但是导入至mysql不会错乱）" class="headerlink" title="案列 ：统计服装制造商主要城市分布 （因为hive字段与值对应错乱，但是导入至mysql不会错乱）"></a><code>案列</code> ：统计服装制造商主要城市分布 （因为hive字段与值对应错乱，但是导入至mysql不会错乱）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select count as addr,count(count)  from commodity20190315 GROUP BY count;</span><br><span class="line">广东广州	361</span><br><span class="line">浙江杭州	94</span><br><span class="line">广东深圳	87</span><br><span class="line">上海	76</span><br><span class="line">广东东莞	74</span><br><span class="line">江苏苏州	52</span><br><span class="line">浙江嘉兴	24</span><br><span class="line">广东佛山	22</span><br><span class="line">福建泉州	15</span><br><span class="line">北京	14</span><br><span class="line">天津	13</span><br><span class="line">四川成都	12</span><br><span class="line"></span><br><span class="line">....... 省略</span><br></pre></td></tr></table></figure>
<p><code>结果</code>：这是对一千多条的抽样调查，由此可见我们平时的衣物制造商地点<code>广东广州</code>居多。</p>
<h3 id="Sqoop-将分析后HIVE数据导出至MYSQL-主要思想："><a href="#Sqoop-将分析后HIVE数据导出至MYSQL-主要思想：" class="headerlink" title="Sqoop 将分析后HIVE数据导出至MYSQL 主要思想："></a>Sqoop 将分析后HIVE数据导出至MYSQL <code>主要思想</code>：</h3><blockquote>
<p>sqoop export –connect jdbc:mysql://IP地址:3306/mall –username root  –password 123456 –table commodity20190315 –export-dir /hivedata/warehouse/hive.db/commodity20190314 –input-fields-terminated-by ‘,’ –input-null-string ‘\N’ –input-null-non-string ‘\N’</p>
</blockquote>
<blockquote>
<p>此命令是经过一下错误原因完善出来的。</p>
</blockquote>
<p><code>--export-dir</code>：代表dfs文件目录，则是hive存储数据的地方<br><img data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/dfs1.jpg" class="lozad"></p>
<p><code>错误原因1</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">19/03/15 09:20:25 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.</span><br><span class="line">19/03/15 09:20:25 ERROR tool.BaseSqoopTool: Error parsing arguments for export:</span><br><span class="line">19/03/15 09:20:25 ERROR tool.BaseSqoopTool: Unrecognized argument: –input-null-string</span><br><span class="line">19/03/15 09:20:25 ERROR tool.BaseSqoopTool: Unrecognized argument: \N</span><br><span class="line">19/03/15 09:20:25 ERROR tool.BaseSqoopTool: Unrecognized argument: –input-null-non-string</span><br><span class="line">19/03/15 09:20:25 ERROR tool.BaseSqoopTool: Unrecognized argument: \N</span><br><span class="line">19/03/15 09:20:25 ERROR tool.BaseSqoopTool: Unrecognized argument: –input-fields-terminated-by</span><br></pre></td></tr></table></figure></p>
<p><code>解决方式</code> ：命令输入错误，注意“-connect”应该是“–connect”杠</p>
<p><code>错误原因2</code><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">19/03/15 09:41:47 ERROR mapreduce.TextExportMapper: Exception:</span><br><span class="line">java.lang.RuntimeException: Can&apos;t parse input data: &apos;2019春季新款chic条纹套头毛衣女装学生韩版宽松显瘦百搭长袖上衣,39.98,42.98,广东 广州,350&apos;</span><br><span class="line">	at commodity20190314.__loadFromFields(commodity20190314.java:487)</span><br><span class="line">	at commodity20190314.parse(commodity20190314.java:386)</span><br><span class="line">	at org.apache.sqoop.mapreduce.TextExportMapper.map(TextExportMapper.java:89)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.Exception: java.io.IOException: Can&apos;t export data, please check failed map task logs</span><br><span class="line">	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)</span><br><span class="line">	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)</span><br><span class="line">Caused by: java.io.IOException: Can&apos;t export data, please check failed map task logs</span><br><span class="line">	at org.apache.sqoop.mapreduce.TextExportMapper.map(TextExportMapper.java:122)</span><br><span class="line">	at org.apache.sqoop.mapreduce.TextExportMapper.map(TextExportMapper.java:39)</span><br><span class="line">	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)</span><br><span class="line">	at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)</span><br><span class="line">	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:745)</span><br></pre></td></tr></table></figure>
<p><code>解决方式</code> ：检查数据是否包含“ ”空格，去掉空格，hive默认分割符–input-fields-terminated-by ‘,’，后续发现mysql表多了id，hive没有导致转码出错。</p>
<p><code>成功将HIVE数据导入MYSQL</code><br><img data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/hiveToMysql.jpg" class="lozad"></p>
<h2 id="统计-分析"><a href="#统计-分析" class="headerlink" title="统计/分析"></a>统计/分析</h2><blockquote>
<p>因数据量较小，则想利用python爬取数据，数据量偏少。则通过第三方地址下载。</p>
</blockquote>
<h3 id="爬取今日头条"><a href="#爬取今日头条" class="headerlink" title="爬取今日头条"></a>爬取今日头条</h3><blockquote>
<p>今日头条每天新闻信息在100条左右，最多抓取5天之内的数据。数据量极少。</p>
</blockquote>
<p><img data-src="http://updatecg.oss-cn-beijing.aliyuncs.com/%E5%A4%B4%E6%9D%A1.jpg" class="lozad"></p>
<h3 id="HIVE数据分析"><a href="#HIVE数据分析" class="headerlink" title="HIVE数据分析"></a>HIVE数据分析</h3><p><code>数据集资源来源</code>:<a href="http://dataju.cn/Dataju/web/home" target="_blank" rel="noopener">http://dataju.cn/Dataju/web/home</a> 里面包含各种类数据集M-T级文件不等。是一个自娱自玩数据来源的好地址。</p>
<blockquote>
<p>总条数 <code>14270481</code> 条</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select count(*) from commodity20190320;</span><br><span class="line">WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Query ID = root_20190320095041_1829fe55-336b-4481-a869-0b24ea274854</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2019-03-20 09:50:43,908 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-03-20 09:50:45,926 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2019-03-20 09:50:46,936 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local1948148359_0001</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage-Stage-1:  HDFS Read: 4150522476 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">14270481</span><br><span class="line">Time taken: 6.276 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<h4 id="按时间动态分区"><a href="#按时间动态分区" class="headerlink" title="按时间动态分区"></a>按时间动态分区</h4><blockquote>
<p><code>commodity20190320</code> 此表是通过csv导入的全量数据，包含了时间段。</p>
</blockquote>
<p><code>使用动态分区需要注意设定以下参数</code>：</p>
<ul>
<li>hive.exec.dynamic.partition  <ul>
<li><code>默认值</code>：false  </li>
<li><code>是否开启动态分区功能</code>: 默认false关闭</li>
</ul>
</li>
<li>hive.exec.dynamic.partition.mode  <ul>
<li><code>默认值</code>：strict  </li>
<li><code>动态分区的模式</code>，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。</li>
</ul>
</li>
<li>hive.exec.max.dynamic.partitions.pernode  <ul>
<li><code>默认值</code>：100</li>
<li>在每个执行MR的节点上，最大可以创建多少个动态分区。</li>
<li>该参数需要根据实际的数据来设定。</li>
<li>比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</li>
</ul>
</li>
<li>hive.exec.max.dynamic.partitions<ul>
<li><code>默认值</code>：1000</li>
<li>在所有执行MR的节点上，最大一共可以创建多少个动态分区。</li>
</ul>
</li>
<li>hive.exec.max.created.files<ul>
<li><code>默认值</code>：100000</li>
<li>整个MR Job中，最大可以创建多少个HDFS文件。</li>
<li>一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">//查看表结构</span><br><span class="line">    hive&gt; desc commodity20190320;</span><br><span class="line">    OK</span><br><span class="line">    affiliatedbasenum   	string              	                    </span><br><span class="line">    locationid          	string              	                    </span><br><span class="line">    pickupdate          	string              	                    </span><br><span class="line">    dispatchingbasenum  	string              	                    </span><br><span class="line">    Time taken: 0.044 seconds, Fetched: 4 row(s)</span><br><span class="line"></span><br><span class="line">//创建按月按天分区表</span><br><span class="line">    hive&gt; CREATE TABLE commodity_partitioned (</span><br><span class="line">        &gt; affiliatedbasenum STRING,</span><br><span class="line">        &gt; locationid STRING,</span><br><span class="line">        &gt; dispatchingbasenum STRING</span><br><span class="line">        &gt; ) PARTITIONED BY (month STRING,day STRING)</span><br><span class="line">        &gt; stored AS textfile;</span><br><span class="line">    OK</span><br><span class="line">    Time taken: 0.238 seconds</span><br><span class="line"></span><br><span class="line">//设置动态分区属性</span><br><span class="line">    hive&gt; SET hive.exec.dynamic.partition=true;  </span><br><span class="line">    hive&gt; SET hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">    hive&gt; SET hive.exec.max.dynamic.partitions.pernode = 1000;</span><br><span class="line">    hive&gt; SET hive.exec.max.dynamic.partitions=1000;</span><br><span class="line"></span><br><span class="line">//时间格式 pickupdate = &quot;5/31/2014 23:59:00&quot; 按天分区则获取年月日即可。利用substr函数：substr(affiliatedbasenum,2,1) AS month,substr(affiliatedbasenum,2,9) AS day</span><br><span class="line">//向分区添加数据</span><br><span class="line">    hive&gt; INSERT overwrite TABLE commodity_partitioned PARTITION (month,day)</span><br><span class="line">        &gt; SELECT locationid,pickupdate,dispatchingbasenum,substr(affiliatedbasenum,2,1) AS month,substr(affiliatedbasenum,2,9) AS day</span><br><span class="line">        &gt; FROM commodity20190320;</span><br></pre></td></tr></table></figure>
<p><img data-src="http://updatecg.oss-cn-beijing.aliyuncs.com/hive_partitions.gif" class="lozad"></p>
<h4 id="为外部表挂载分区"><a href="#为外部表挂载分区" class="headerlink" title="为外部表挂载分区"></a>为外部表挂载分区</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---------------------------------java代码-----------------------------------------</span><br><span class="line">    /**</span><br><span class="line">     * @param tableName 外部表名</span><br><span class="line">     * @param yyyyMMdd  分区标识</span><br><span class="line">     * @param dfsUrl    dfs地址</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public void loadOuterHiveDataPartitions(String tableName, String yyyyMMdd, String dfsUrl) &#123;</span><br><span class="line">        StringBuffer sql = new StringBuffer(&quot;alter table &quot; + tableName);</span><br><span class="line">        sql.append(&quot; add partition (day=&apos;&quot; + yyyyMMdd + &quot;&apos;) location &apos;&quot; + dfsUrl + yyyyMMdd + &quot;/&apos;&quot;);</span><br><span class="line">        Log.info(&quot;Load data into OuterHiveDataPartitions successfully...&quot;);</span><br><span class="line">        try &#123;</span><br><span class="line">            hiveJdbcTemplate.execute(sql.toString());</span><br><span class="line">        &#125; catch (DataAccessException dae) &#123;</span><br><span class="line">            Log.error(dae.fillInStackTrace());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---------------------------------Sql-----------------------------------------</span><br><span class="line">  alter table uber_outer_partitioned add partition (day=&apos;2019-03-21&apos;) location &apos;/data/outerClientSummary/2019-03-21&apos;</span><br></pre></td></tr></table></figure>
<p><img data-src="http://updatecg.oss-cn-beijing.aliyuncs.com/hive_outer_partitions.jpg" class="lozad"></p>
<p><code>注意</code>：分区数据支持sql查询</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于大数据初学者的我，这才是我的第一步，都说万事开头难，坚持吧。</p>
<ul>
<li>知道如何把已有的数据采集到HDFS上，包括离线采集和实时采集；</li>
<li>知道sqoop是HDFS和其他数据源之间的数据交换工具,支持把数据在HDFS\HIVE\MYSQL互相传输；</li>
<li>知道Hadoop的MRV1与Yarn(MRV2)的区别，最主要的单点故障以及性能大大提升。<ul>
<li>JobTracker被RescourceManager替换</li>
<li>每一个节点的TaskTacker被NodeManager替换</li>
<li>Yarn大大减小了 JobTracker（也就是现在的 ResourceManager）的资源消耗。</li>
<li>监测每一个 Job 子任务 (tasks) 状态的程序分布式化了</li>
</ul>
</li>
<li>Hive外部表被删除时，不会删除元数据，可以直接在外部表基础啊上创建分区表。</li>
<li>Hive一般作为数据仓库，几乎不会被用作与OLAP操作<ul>
<li>原因则在于hive数据量庞大时查询速度太慢.<code>下一章则会着重介绍</code>.</li>
</ul>
</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">陈 武</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://www.updatecg.xin/2019/03/13/大数据案例之HDFS-HIVE/">http://www.updatecg.xin/2019/03/13/大数据案例之HDFS-HIVE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.updatecg.xin">我的学习记录</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop    </a><a class="post-meta__tags" href="/tags/Sqoop/">Sqoop    </a><a class="post-meta__tags" href="/tags/Hive/">Hive    </a><a class="post-meta__tags" href="/tags/Python/">Python    </a></div><div class="post_share"><div class="social-share" data-image="https://updatecg.oss-cn-beijing.aliyuncs.com/timg.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-buttom"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="img/cgs.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="img/zhifubao.png"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/03/21/大数据案例之HDFS-HIVE-Spark/"><img class="prev_cover lozad" data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/wallhaven-gj2j2l.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>大数据案例之HDFS-HIVE-Spark</span></div></a></div><div class="next-post pull-right"><a href="/2019/03/13/sqoop安装部署问题事项/"><img class="next_cover lozad" data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/wallhaven-gj2j2l.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>sqoop安装部署问题事项</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/03/21/大数据案例之HDFS-HIVE-Spark/" title="大数据案例之HDFS-HIVE-Spark"><img class="relatedPosts_cover lozad" data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/wallhaven-gj2j2l.jpg"><div class="relatedPosts_title">大数据案例之HDFS-HIVE-Spark</div></a></div><div class="relatedPosts_item"><a href="/2019/03/13/sqoop安装部署问题事项/" title="sqoop安装部署问题事项"><img class="relatedPosts_cover lozad" data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/wallhaven-gj2j2l.jpg"><div class="relatedPosts_title">sqoop安装部署问题事项</div></a></div><div class="relatedPosts_item"><a href="/2018/06/20/Hadoop环境搭建/" title="Hadoop环境搭建"><img class="relatedPosts_cover lozad" data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/wallhaven-gj2j2l.jpg"><div class="relatedPosts_title">Hadoop环境搭建</div></a></div><div class="relatedPosts_item"><a href="/2018/06/20/利用HDFS、RabbitMQ、MongoDB实现统计/" title="利用HDFS、RabbitMQ、MongoDB实现统计"><img class="relatedPosts_cover lozad" data-src="https://updatecg.oss-cn-beijing.aliyuncs.com/wallhaven-gj2j2l.jpg"><div class="relatedPosts_title">利用HDFS、RabbitMQ、MongoDB实现统计</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'QWEsCLTRaW2S9rUPn8Mq28CU-gzGzoHsz',
  appKey:'0b5xzRKTdodumxJJ1wYXcLWe',
  placeholder:'欢迎建议留言...',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2021 By 陈 武</div><div class="icp"><a href="https://www.beian.miit.gov.cn/state/outPortal/loginPortal.action" target="_blank"><img class="lozad" data-src="https://www.larscheng.com/img/beian.png" onerror="onerror=null;src='https://www.larscheng.com/img/beian.png'" style="padding:0px;vertical-align: text-bottom;"><span>蜀ICP备19024604号</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="夜间模式"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据采集"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">数据采集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据处理"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">数据处理</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#CSV-数据加载入Hadoop-部分代码"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">CSV 数据加载入Hadoop  部分代码</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#将dfs文件加载入hive-部分代码"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">将dfs文件加载入hive 部分代码</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#利用外部表加载dfs数据至分区表"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text">利用外部表加载dfs数据至分区表</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#外部表好处"><span class="toc_mobile_items-number">2.2.1.1.</span> <span class="toc_mobile_items-text">外部表好处</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#Hive创建外部表"><span class="toc_mobile_items-number">2.2.1.2.</span> <span class="toc_mobile_items-text">Hive创建外部表</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#HIVE分析数据"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">HIVE分析数据</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#案列-：统计服装制造商主要城市分布-（因为hive字段与值对应错乱，但是导入至mysql不会错乱）"><span class="toc_mobile_items-number">2.3.1.</span> <span class="toc_mobile_items-text">案列 ：统计服装制造商主要城市分布 （因为hive字段与值对应错乱，但是导入至mysql不会错乱）</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Sqoop-将分析后HIVE数据导出至MYSQL-主要思想："><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text">Sqoop 将分析后HIVE数据导出至MYSQL 主要思想：</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#统计-分析"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">统计/分析</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#爬取今日头条"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">爬取今日头条</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#HIVE数据分析"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">HIVE数据分析</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#按时间动态分区"><span class="toc_mobile_items-number">3.2.1.</span> <span class="toc_mobile_items-text">按时间动态分区</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#为外部表挂载分区"><span class="toc_mobile_items-number">3.2.2.</span> <span class="toc_mobile_items-text">为外部表挂载分区</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#总结"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">总结</span></a></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.6" zindex="-1" data-click="false"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/baidupush.js"> </script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script></body></html>